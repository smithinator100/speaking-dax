# Speaking DAX - Audio to Viseme Converter

Convert audio files to JSON viseme maps for lip-sync animation using either Rhubarb Lip Sync or Gentle forced aligner.

## Table of Contents

- [Overview](#overview)
- [Which Tool Should I Use?](#which-tool-should-i-use)
- [Installation](#installation)
- [Quick Start](#quick-start)
- [Usage](#usage)
- [Viseme Codes](#viseme-codes)
- [Output Format](#output-format)
- [API Reference](#api-reference)
- [Detailed Comparison](#detailed-comparison)
- [Troubleshooting](#troubleshooting)
- [Examples](#examples)

## Overview

This project provides four powerful approaches for generating lip-sync animation data:

- **[Rhubarb Lip Sync](https://github.com/DanielSWolf/rhubarb-lip-sync)** - Fast phonetic analysis (no transcript required)
- **[WhisperX](https://github.com/m-bain/whisperX)** - AI-powered transcription with phoneme alignment (no transcript required)
- **[Gentle](https://github.com/lowerquality/gentle)** - Accurate forced alignment (requires transcript)
- **[Hybrid Pipeline](lip-sync-libraries/hybrid/)** ‚≠ê - WhisperX ‚Üí Gentle (best accuracy + convenience)

All tools generate identical JSON output formats, allowing you to seamlessly switch between them or use multiple depending on your needs.

### üöÄ Recommended: Hybrid Pipeline

The **Hybrid Pipeline** combines WhisperX's auto-transcription with Gentle's precise alignment for **maximum accuracy without manual transcription**. This is the recommended approach for production-quality results.

## Which Tool Should I Use?

| Feature | Rhubarb | WhisperX | Gentle | **Hybrid** ‚≠ê |
|---------|---------|----------|--------|---------------|
| **Speed** | ‚ö° Fast (~5-10s/min) | ‚ö°‚ö° Very Fast (~10-20s/min) | üê¢ Moderate (~30-60s/min) | üê¢ Moderate (~40-65s/min) |
| **Accuracy** | ‚úÖ Good (~85-90%) | ‚≠ê‚≠ê Excellent (~90-95%) | ‚≠ê Excellent (~95-98%) | **‚≠ê‚≠ê‚≠ê Excellent (~95-98%)** |
| **Transcript Required** | ‚ùå No | ‚ùå No (auto-generates) | ‚úÖ Yes | **‚ùå No (auto-generates)** |
| **Setup Complexity** | üü¢ Simple (binary) | üü° Moderate (Python) | üî¥ Complex (Python + Kaldi) | üî¥ Complex (Both) |
| **File Size** | ~20 MB | ~75 MB - 1.5 GB | ~500 MB | ~575 MB - 2 GB |
| **Multi-language** | ‚ùå English only | ‚úÖ 90+ languages | ‚úÖ Multiple languages | **‚úÖ 90+ languages** |
| **GPU Support** | ‚ùå No | ‚úÖ Yes (CUDA) | ‚ùå No | ‚úÖ Yes (WhisperX step) |
| **Best For** | Quick lip sync | AI transcription + visemes | Known transcript precision | **Production quality** |

### Use Rhubarb when:
- ‚úÖ You don't have a transcript
- ‚úÖ You need the fastest results
- ‚úÖ Setup simplicity is critical
- ‚úÖ Working with music/non-speech
- ‚úÖ Real-time applications
- ‚úÖ Prototyping and iteration

### Use WhisperX when:
- ‚úÖ You don't have a transcript (auto-generates)
- ‚úÖ You need high accuracy without manual transcripts
- ‚úÖ Multi-language support required
- ‚úÖ You have a GPU for faster processing
- ‚úÖ Modern AI-based approach preferred
- ‚úÖ You need both transcription and viseme data

### Use Gentle when:
- ‚úÖ You have an accurate transcript
- ‚úÖ You need maximum phoneme timing precision
- ‚úÖ You want word-level alignment with known text
- ‚úÖ Working with clear dialogue
- ‚úÖ Production quality with transcript
- ‚úÖ You can afford longer processing time

### Use Hybrid Pipeline when: ‚≠ê **RECOMMENDED**
- ‚úÖ You need maximum accuracy
- ‚úÖ No manual transcription available
- ‚úÖ Production-quality results required
- ‚úÖ Best overall quality/convenience trade-off
- ‚úÖ Multi-language support needed
- ‚úÖ Willing to invest in setup complexity

## Installation

### 1. Install Node.js Dependencies

```bash
npm install
```

### 2. Choose Your Tool

#### Option A: Rhubarb (Recommended for Beginners)

**macOS (via Homebrew):**
```bash
brew install rhubarb-lip-sync
```

**Manual Installation:**
1. Download from [GitHub Releases](https://github.com/DanielSWolf/rhubarb-lip-sync/releases)
2. Extract the executable
3. Add to PATH or place in `lip-sync-libraries/rhubarb/`

**Verify Installation:**
```bash
rhubarb --version
```

#### Option B: WhisperX (Recommended for AI-based approach)

**Prerequisites:**
- Python 3.8+
- (Optional) NVIDIA GPU with CUDA for faster processing

**Quick Setup:**
```bash
# Install Python dependencies
cd lip-sync-libraries/whisperx
pip install -r requirements.txt

# Or install manually
pip install whisperx torch torchaudio

# Verify installation
python3 -c "import whisperx; print('WhisperX installed successfully')"
```

**For GPU acceleration (optional but recommended):**
```bash
# Visit https://pytorch.org/get-started/locally/ for your specific CUDA version
# Example for CUDA 11.8:
pip install torch torchaudio --index-url https://download.pytorch.org/whl/cu118
```

See [WhisperX README](lip-sync-libraries/whisperx/README.md) for detailed setup instructions.

#### Option C: Gentle (For Advanced Users)

**Prerequisites:**
- Python 3.7+
- ffmpeg

**Quick Setup:**
```bash
# Install Python and ffmpeg
brew install python3 ffmpeg  # macOS
# OR
sudo apt-get install python3 python3-pip ffmpeg  # Ubuntu

# Clone Gentle
git clone https://github.com/lowerquality/gentle.git
cd gentle

# Install dependencies
pip3 install -r requirements.txt
./install.sh

# Set environment variable (add to ~/.zshrc or ~/.bashrc)
export GENTLE_RESOURCES_ROOT="$(pwd)"

# Test installation
python3 align.py examples/data/lucier.mp3 examples/data/lucier.txt
```

**Docker (Easiest):**
```bash
docker pull lowerquality/gentle
docker run -p 8765:8765 lowerquality/gentle
```

#### Option D: Hybrid Pipeline ‚≠ê (Recommended for Production)

**Prerequisites:**
- All requirements from WhisperX (Option B)
- All requirements from Gentle (Option C)

**Quick Setup:**
```bash
# 1. Install WhisperX
pip install whisperx

# 2. Install Gentle
git clone https://github.com/lowerquality/gentle.git
cd gentle
./install.sh
export GENTLE_RESOURCES_ROOT="$(pwd)"

# 3. Verify installation
node lip-sync-libraries/hybrid/hybrid-to-viseme.js --help
```

See [Hybrid Pipeline README](lip-sync-libraries/hybrid/README.md) for detailed setup instructions.

## Quick Start

### Test the Lip Sync Visualization

Open `index.html` in your browser to see a live demo:

```bash
open index.html
```

This interactive demo loads sample audio and viseme JSON, displaying synchronized mouth shapes in real-time.

### Generate Your First Viseme Map

**Using Hybrid Pipeline (Recommended):** ‚≠ê
```bash
# Best accuracy + convenience (auto-transcribes with Gentle precision)
node lip-sync-libraries/hybrid/hybrid-to-viseme.js audio.mp3

# Or use the convenience script
./lip-sync-libraries/hybrid/convert.sh audio.mp3

# Or use npm script
npm run convert:hybrid audio.mp3
```

**Using Rhubarb:**
```bash
# Using the convenience script
./lip-sync-libraries/rhubarb/convert.sh audio.mp3

# Or direct node usage
node lip-sync-libraries/rhubarb/mp3-to-viseme.js audio.mp3

# With dialog text (improves accuracy)
node lip-sync-libraries/rhubarb/mp3-to-viseme.js audio.mp3 output.json dialog.txt
```

**Using WhisperX:**
```bash
# No transcript required (auto-transcribes)
node lip-sync-libraries/whisperx/whisperx-to-viseme.js audio.mp3

# With larger model for better accuracy
node lip-sync-libraries/whisperx/whisperx-to-viseme.js audio.mp3 output.json --model large-v2
```

**Using Gentle:**
```bash
# Requires transcript
node lip-sync-libraries/gentle/gentle-to-viseme.js audio.mp3 transcript.txt
```

## Usage

### Hybrid Pipeline - Best Accuracy + Convenience ‚≠ê

**Command Line:**
```bash
# Basic usage (auto-transcribes, no manual work needed)
node lip-sync-libraries/hybrid/hybrid-to-viseme.js audio.mp3

# With custom output
node lip-sync-libraries/hybrid/hybrid-to-viseme.js audio.mp3 output.json

# With larger WhisperX model for better accuracy
node lip-sync-libraries/hybrid/hybrid-to-viseme.js audio.mp3 output.json --whisper-model large-v2

# Specify language for faster processing
node lip-sync-libraries/hybrid/hybrid-to-viseme.js audio.mp3 output.json --whisper-language en

# Use GPU acceleration (requires CUDA)
node lip-sync-libraries/hybrid/hybrid-to-viseme.js audio.mp3 output.json --whisper-device cuda

# Fast mode (less accurate but faster)
node lip-sync-libraries/hybrid/hybrid-to-viseme.js audio.mp3 output.json --gentle-fast

# Using convenience script
./lip-sync-libraries/hybrid/convert.sh audio.mp3

# Using npm script
npm run convert:hybrid audio.mp3
```

**As a Module:**
```javascript
import { convertAudioToViseme } from './lip-sync-libraries/hybrid/hybrid-to-viseme.js';

const visemeData = await convertAudioToViseme('audio.mp3', 'visemes.json', {
  // WhisperX options
  whisperModel: 'base',           // Model: tiny, base, small, medium, large-v2
  whisperLanguage: 'auto',        // Language code or 'auto' for detection
  whisperDevice: 'cpu',           // 'cpu' or 'cuda' (GPU)
  whisperBatchSize: 16,
  whisperComputeType: 'float32',
  
  // Gentle options
  gentleConservative: true,       // More accurate (default)
  gentleThreads: 8,               // Number of threads
  gentleDisfluency: false,        // Include uh, um, etc.
  
  // General options
  minDuration: 0.03,              // Min viseme duration
  saveTranscript: true,           // Save transcript to file
  transcriptPath: 'custom.txt',   // Custom transcript path
  keepTempFiles: false,           // Keep temp files
  verbose: false                  // Verbose logging
});

console.log(`Transcript: ${visemeData.metadata.transcript}`);
console.log(`Generated ${visemeData.mouthCues.length} viseme cues`);
```

See [Hybrid Pipeline README](lip-sync-libraries/hybrid/README.md) for detailed documentation.

### Rhubarb - Fast, No Transcript

**Command Line:**
```bash
# Basic usage
node lip-sync-libraries/rhubarb/mp3-to-viseme.js audio.mp3

# With custom output
node lip-sync-libraries/rhubarb/mp3-to-viseme.js audio.mp3 output.json

# With dialog text (improves accuracy)
node lip-sync-libraries/rhubarb/mp3-to-viseme.js audio.mp3 output.json dialog.txt

# Using npm script
npm run convert audio.mp3
```

**As a Module:**
```javascript
import { convertMp3ToViseme } from './lip-sync-libraries/rhubarb/mp3-to-viseme.js';

const visemeData = await convertMp3ToViseme('audio.mp3', 'visemes.json', {
  dialogFile: 'transcript.txt',  // Optional: improves accuracy
  rhubarbPath: 'rhubarb',        // Optional: custom path
  recognizer: 'phonetic'         // or 'pocketSphinx'
});
```

### WhisperX - AI-Powered, No Transcript Required

**Command Line:**
```bash
# Basic usage (no transcript required)
node lip-sync-libraries/whisperx/whisperx-to-viseme.js audio.mp3

# With custom output
node lip-sync-libraries/whisperx/whisperx-to-viseme.js audio.mp3 output.json

# With larger model for better accuracy
node lip-sync-libraries/whisperx/whisperx-to-viseme.js audio.mp3 output.json --model large-v2

# Specify language for faster processing
node lip-sync-libraries/whisperx/whisperx-to-viseme.js audio.mp3 output.json --language en

# Use GPU acceleration (requires CUDA)
node lip-sync-libraries/whisperx/whisperx-to-viseme.js audio.mp3 output.json --device cuda

# Using npm script
npm run convert:whisperx audio.mp3
```

**As a Module:**
```javascript
import { convertAudioToViseme } from './lip-sync-libraries/whisperx/whisperx-to-viseme.js';

const visemeData = await convertAudioToViseme('audio.mp3', 'visemes.json', {
  model: 'base',           // Model: tiny, base, small, medium, large-v2
  language: 'auto',        // Language code or 'auto' for detection
  device: 'cpu',           // 'cpu' or 'cuda' (GPU)
  batchSize: 16,           // Batch size
  computeType: 'float32',  // 'float32', 'float16', 'int8'
  minDuration: 0.03,       // Min viseme duration
  verbose: false           // Verbose logging
});
```

### Gentle - Accurate, Requires Transcript

**Command Line:**
```bash
# Basic usage (requires transcript)
node lip-sync-libraries/gentle/gentle-to-viseme.js audio.mp3 transcript.txt

# With custom output
node lip-sync-libraries/gentle/gentle-to-viseme.js audio.mp3 transcript.txt output.json

# Using npm script
npm run convert:gentle audio.mp3 transcript.txt
```

**As a Module:**
```javascript
import { convertAudioToViseme } from './lip-sync-libraries/gentle/gentle-to-viseme.js';

const visemeData = await convertAudioToViseme(
  'audio.mp3',
  'transcript.txt',
  'visemes.json',
  {
    conservative: false,      // More accurate but slower
    nthreads: 4,             // Number of threads
    disfluency: false,       // Include uh, um, etc.
    minDuration: 0.03        // Minimum viseme duration
  }
);
```

### Transcript File Format

For Gentle, create a plain text file with the spoken words:

**transcript.txt:**
```
I am not interested in speech sounds as such, but in the
possibilities of their use in music.
```

**Tips:**
- Use plain text (no special formatting)
- Match spoken words exactly
- Include filler words if spoken (uh, um)
- Don't include timestamps or speaker labels

## Viseme Codes

Both tools use the Preston Blair phoneme set (Rhubarb standard):

| Code | Mouth Shape | Description | Example Sounds |
|------|-------------|-------------|----------------|
| **X** | Rest | Relaxed/neutral position | Silence, pauses |
| **A** | Closed lips | Slight pressure between lips | **M**om, **B**at, **P**at |
| **B** | Slightly open | Clenched teeth | **S**ee, **T**ea, **K**ey |
| **C** | Open medium | Medium open mouth | M**e**n, B**a**t |
| **D** | Wide open | Wide open mouth | F**a**ther, C**a**r |
| **E** | Rounded | Slightly rounded | **O**ff, B**ir**d |
| **F** | Puckered | Puckered lips | Y**ou**, Sh**ow**, **W**ay |
| **G** | Teeth on lip | Upper teeth on lower lip | **F**or, **V**ery |
| **H** | Tongue up | Tongue raised | **L**ong, Te**ll** |

### Mouth Shape Files

The `mouth-shapes-png/` directory contains PNG images for each viseme:
- `neutral.png` - X (rest position)
- `M.png` - A (closed lips)
- `Ee.png` - B (clenched teeth)
- `Aa.png` - C (open medium)
- `D.png` - D (wide open)
- `Uh.png` - E (rounded)
- `W-oo.png` - F (puckered)
- `F.png` - G (teeth on lip)
- `L.png` - H (tongue up)

## Output Format

All tools generate identical JSON format:

```json
{
  "metadata": {
    "soundFile": "audio.wav",
    "duration": 3.93,
    "transcript": "Hello world",
    "source": "rhubarb" | "whisperx" | "gentlejs" | "hybrid-whisperx-gentle",
    "language": "en",
    "pipeline": {
      "step1": "WhisperX transcription",
      "step2": "Gentle forced alignment",
      "whisperModel": "base",
      "whisperLanguage": "en",
      "gentleConservative": true
    }
  },
  "mouthCues": [
    { "start": 0.00, "end": 0.07, "value": "X" },
    { "start": 0.07, "end": 0.16, "value": "C" },
    { "start": 0.16, "end": 0.30, "value": "B" }
  ]
}
```

**Fields:**
- `metadata.soundFile` - Original audio file path
- `metadata.duration` - Total duration in seconds
- `metadata.transcript` - Transcript text (WhisperX, Gentle, and Hybrid)
- `metadata.language` - Detected language (WhisperX and Hybrid)
- `metadata.source` - Tool used: `"rhubarb"`, `"whisperx"`, `"gentlejs"`, or `"hybrid-whisperx-gentle"`
- `metadata.pipeline` - Pipeline details (Hybrid only)
- `mouthCues` - Array of timed mouth shapes

## API Reference

### Hybrid Pipeline API ‚≠ê

#### `convertAudioToViseme(audioPath, outputJsonPath, options)`

**Parameters:**
- `audioPath` (string): Path to input audio file (any format)
- `outputJsonPath` (string): Path for output JSON file
- `options` (object):
  - **WhisperX Options:**
    - `whisperModel` (string): Model size - 'tiny', 'base', 'small', 'medium', 'large-v2'
    - `whisperLanguage` (string): Language code (e.g., 'en', 'es') or 'auto'
    - `whisperDevice` (string): 'cpu' or 'cuda' for GPU acceleration
    - `whisperBatchSize` (number): Batch size for processing
    - `whisperComputeType` (string): 'float32', 'float16', 'int8'
  - **Gentle Options:**
    - `gentleResourcesPath` (string): Path to Gentle directory (default: env var)
    - `gentleThreads` (number): Number of threads (default: CPU count)
    - `gentleConservative` (boolean): More accurate alignment (default: true)
    - `gentleDisfluency` (boolean): Include uh, um, etc. (default: false)
  - **General Options:**
    - `minDuration` (number): Min viseme duration in seconds (default: 0.03)
    - `saveTranscript` (boolean): Save transcript to file (default: true)
    - `transcriptPath` (string): Custom transcript save path
    - `keepTempFiles` (boolean): Keep temporary files (default: false)
    - `verbose` (boolean): Enable verbose logging (default: false)

**Returns:** Promise<Object> - Viseme map data with transcript and pipeline metadata

**Example:**
```javascript
const visemeData = await convertAudioToViseme('audio.mp3', 'output.json', {
  whisperModel: 'large-v2',
  gentleConservative: true,
  verbose: true
});
```

### Rhubarb API

#### `convertMp3ToViseme(inputMp3Path, outputJsonPath, options)`

**Parameters:**
- `inputMp3Path` (string): Path to input audio file
- `outputJsonPath` (string): Path for output JSON file
- `options` (object):
  - `rhubarbPath` (string): Path to Rhubarb executable
  - `dialogFile` (string): Optional dialog text file
  - `recognizer` (string): 'phonetic' or 'pocketSphinx'
  - `exportFormat` (string): 'json', 'tsv', 'xml', 'dat'

**Returns:** Promise<Object> - Viseme map data

### WhisperX API

#### `convertAudioToViseme(audioPath, outputJsonPath, options)`

**Parameters:**
- `audioPath` (string): Path to input audio file (any format)
- `outputJsonPath` (string): Path for output JSON file
- `options` (object):
  - `model` (string): Model size - 'tiny', 'base', 'small', 'medium', 'large-v2'
  - `language` (string): Language code (e.g., 'en', 'es') or 'auto'
  - `device` (string): 'cpu' or 'cuda' for GPU acceleration
  - `batchSize` (number): Batch size for processing
  - `computeType` (string): 'float32', 'float16', 'int8'
  - `minDuration` (number): Min viseme duration in seconds
  - `verbose` (boolean): Enable verbose logging

**Returns:** Promise<Object> - Viseme map data with transcript

### Gentle API

#### `convertAudioToViseme(audioPath, transcriptPath, outputJsonPath, options)`

**Parameters:**
- `audioPath` (string): Path to input audio file
- `transcriptPath` (string): Path to transcript text file
- `outputJsonPath` (string): Path for output JSON file
- `options` (object):
  - `gentleResourcesPath` (string): Path to Gentle directory
  - `nthreads` (number): Number of threads (default: CPU count)
  - `conservative` (boolean): More accurate alignment
  - `disfluency` (boolean): Include uh, um, etc.
  - `minDuration` (number): Min viseme duration in seconds

**Returns:** Promise<Object> - Viseme map data

### Shared Utilities

#### `formatVisemeData(visemeData)`
Formats viseme data into readable structure.

#### `getVisemeDescription(viseme)`
Gets human-readable description for viseme code.

## Detailed Comparison

### Performance Benchmarks

Based on testing with 1-minute audio file on MacBook Pro M1:

| Metric | Rhubarb | WhisperX (CPU) | WhisperX (GPU) | Gentle | **Hybrid (CPU)** | **Hybrid (GPU)** |
|--------|---------|----------------|----------------|--------|------------------|------------------|
| Processing Time | 5-8 seconds | 10-20 seconds | 3-5 seconds | 30-45 seconds | **40-65 seconds** | **33-50 seconds** |
| Memory Usage | ~50 MB | ~500 MB - 2 GB | ~1-3 GB | ~500 MB | **~1-2.5 GB** | **~1.5-3.5 GB** |
| CPU Usage | Low-Medium | High | Low | High | **High** | **Medium** |
| Disk Space | ~20 MB | ~75 MB - 1.5 GB | ~75 MB - 1.5 GB | ~500 MB | **~575 MB - 2 GB** | **~575 MB - 2 GB** |
| Cold Start | Instant | 5-10 seconds | 5-10 seconds | 2-3 seconds | **7-13 seconds** | **7-13 seconds** |
| GPU Acceleration | ‚ùå No | ‚ùå No | ‚úÖ Yes | ‚ùå No | ‚úÖ Yes (WhisperX) | ‚úÖ Yes (WhisperX) |

### Accuracy Comparison

**Rhubarb:**
- Phoneme Detection: ~85-90% accurate
- Timing Precision: ¬±50ms typical
- Best With: Clear speech, single speaker
- Struggles With: Multiple speakers, background noise

**WhisperX:**
- Phoneme Detection: ~90-95% accurate
- Timing Precision: ¬±20-30ms typical
- Transcription: ~95-98% accurate (auto-generated)
- Best With: Clear speech, any language, any accent
- Struggles With: Heavy background noise, overlapping speakers

**Gentle:**
- Phoneme Detection: ~95-98% accurate (with transcript)
- Timing Precision: ¬±10-20ms typical
- Best With: Clear speech matching transcript
- Struggles With: Transcript mismatches, heavy accents

**Hybrid Pipeline:** ‚≠ê
- Phoneme Detection: ~95-98% accurate (WhisperX transcript + Gentle alignment)
- Timing Precision: ¬±10-20ms typical (Gentle precision)
- Transcription: ~95-98% accurate (WhisperX auto-generated)
- Best With: Any speech, any language, no transcript needed
- Struggles With: Heavy background noise (affects WhisperX transcription)
- **Advantage:** Combines WhisperX's auto-transcription with Gentle's precision

### Technology Stack

**Rhubarb:**
- Language: C++ (standalone binary)
- Algorithm: Phonetic pattern matching
- Recognition: PocketSphinx (optional) or phonetic rules
- Installation: Single executable download

**WhisperX:**
- Language: Python + PyTorch
- Algorithm: Transformer-based neural network (Whisper)
- Recognition: OpenAI Whisper + forced alignment
- Installation: pip install + automatic model downloads

**Gentle:**
- Language: Python + C++ (Kaldi)
- Algorithm: HMM-based forced alignment
- Recognition: Kaldi ASR with acoustic models
- Installation: Multiple dependencies + model downloads

### Use Case Recommendations

#### Choose Rhubarb For:

‚úÖ **Quick Prototypes**
- Rapid iteration on animations
- Testing lip sync concepts
- No transcript available

‚úÖ **Real-Time Applications**
- Live performance capture
- Interactive applications
- Resource-constrained environments

‚úÖ **Background Audio**
- Music with vocals
- Ambient dialogue
- Sound effects with speech

‚úÖ **Simple Deployment**
- Single binary distribution
- Minimal dependencies
- Cross-platform support

#### Choose Gentle For:

‚úÖ **Production Quality**
- Film/TV animation
- High-quality game cutscenes
- Professional voiceover work

‚úÖ **Known Scripts**
- Scripted dialogue
- Narration with transcript
- Dubbed content

‚úÖ **Detailed Analysis**
- Need word-level timing
- Phoneme-level precision required
- Research applications

‚úÖ **Post-Production**
- Batch processing
- Offline rendering
- Quality over speed

### Hybrid Approach ‚≠ê

The **Hybrid Pipeline** is now built-in and combines the best of WhisperX and Gentle:

```javascript
import { convertAudioToViseme } from './lip-sync-libraries/hybrid/hybrid-to-viseme.js';

// Hybrid Pipeline: WhisperX auto-transcribes, Gentle aligns with precision
const visemeData = await convertAudioToViseme('audio.mp3', 'output.json', {
  whisperModel: 'base',           // Use larger models for better accuracy
  gentleConservative: true,       // Maximum precision
  saveTranscript: true            // Save the auto-generated transcript
});

console.log(`Transcript: ${visemeData.metadata.transcript}`);
console.log(`Viseme cues: ${visemeData.mouthCues.length}`);
```

**Why use the Hybrid Pipeline?**
- ‚úÖ No manual transcription needed (WhisperX auto-generates)
- ‚úÖ Maximum phoneme-level precision (Gentle alignment)
- ‚úÖ Best accuracy without manual work
- ‚úÖ Multi-language support (90+ languages)
- ‚úÖ Production-quality results

**Fallback Strategy:**

For maximum flexibility, you can implement a fallback strategy:

```javascript
async function generateVisemesWithFallback(audioPath) {
  // Try Hybrid Pipeline first (best accuracy)
  if (process.env.GENTLE_RESOURCES_ROOT) {
    try {
      const { convertAudioToViseme } = await import('./lip-sync-libraries/hybrid/hybrid-to-viseme.js');
      return await convertAudioToViseme(audioPath, 'output.json', {
        whisperModel: 'base',
        gentleConservative: true
      });
    } catch (error) {
      console.warn('Hybrid Pipeline failed, falling back to WhisperX only');
    }
  }
  
  // Fall back to WhisperX only (still very good accuracy)
  try {
    const { convertAudioToViseme } = await import('./lip-sync-libraries/whisperx/whisperx-to-viseme.js');
    return await convertAudioToViseme(audioPath, 'output.json');
  } catch (error) {
    console.warn('WhisperX failed, falling back to Rhubarb');
  }
  
  // Final fallback to Rhubarb (fastest, always works)
  const { convertMp3ToViseme } = await import('./lip-sync-libraries/rhubarb/mp3-to-viseme.js');
  return await convertMp3ToViseme(audioPath, 'output.json');
}
```

## Troubleshooting

### Rhubarb Issues

**"Rhubarb executable not found"**
- Install Rhubarb: `brew install rhubarb-lip-sync`
- Or specify path: `rhubarbPath: '/path/to/rhubarb'`

**Poor accuracy:**
- Provide dialog text file
- Use high-quality audio (WAV recommended)
- Reduce background noise

### Gentle Issues

**"Gentle resources path not set"**
- Set environment variable: `export GENTLE_RESOURCES_ROOT=/path/to/gentle`
- Add to ~/.zshrc or ~/.bashrc to persist

**"Python 3 not found"**
- Install Python 3: `brew install python3` (macOS)
- Or: `sudo apt-get install python3` (Ubuntu)

**"align.py not found"**
- Ensure `GENTLE_RESOURCES_ROOT` points to Gentle directory
- Verify: `ls $GENTLE_RESOURCES_ROOT/align.py`

**"Gentle process exited with code 1"**

Possible causes:
1. **Missing language models:** Run `./install.sh` in Gentle directory
2. **Audio format issues:** Try converting to WAV with ffmpeg
3. **Transcript mismatch:** Ensure transcript matches spoken words exactly
4. **Python dependencies:** Run `pip3 install -r requirements.txt`

**Poor alignment quality:**
- Enable conservative mode: `conservative: true`
- Clean up transcript: Remove punctuation, match exact spoken words
- Improve audio quality: Use clear, noise-free audio
- Check sample rate: Gentle works best with 16kHz audio

### General Issues

**Supported audio formats:**
- MP3, WAV, OGG (both tools support common formats)

**ffmpeg errors:**
- Install ffmpeg: `brew install ffmpeg` (macOS)
- Or: `sudo apt-get install ffmpeg` (Ubuntu)

### Performance Optimization

#### Gentle Speed Tips:
1. Use more threads: Set `nthreads` to CPU core count
2. Disable conservative mode: Faster but less accurate
3. Pre-convert audio: Convert to 16kHz mono WAV beforehand
4. Batch processing: Process multiple files in parallel

## Examples

### Web Animation

```javascript
import { convertMp3ToViseme, formatVisemeData } from './lip-sync-libraries/rhubarb/mp3-to-viseme.js';

const visemeData = await convertMp3ToViseme('speech.mp3', 'speech.json');
const cues = formatVisemeData(visemeData);

// Sync with audio playback
const audio = new Audio('speech.mp3');
audio.addEventListener('timeupdate', () => {
  const currentTime = audio.currentTime;
  const currentCue = cues.find(
    (cue, i) => currentTime >= cue.time && 
                (i === cues.length - 1 || currentTime < cues[i + 1].time)
  );
  
  if (currentCue) {
    updateCharacterMouth(currentCue.viseme);
  }
});
```

### Three.js / Unity Integration

Both tools export the same JSON format, making it easy to drive blend shapes or morph targets in 3D character animation.

### Batch Processing

```javascript
import { promises as fs } from 'fs';
import path from 'path';
import { convertMp3ToViseme } from './lip-sync-libraries/rhubarb/mp3-to-viseme.js';

async function batchProcess(audioFiles) {
  const results = [];
  
  for (const audioFile of audioFiles) {
    const outputFile = audioFile.replace(/\.mp3$/, '_visemes.json');
    
    try {
      const visemeData = await convertMp3ToViseme(audioFile, outputFile);
      results.push({ file: audioFile, success: true, cues: visemeData.mouthCues.length });
    } catch (error) {
      results.push({ file: audioFile, success: false, error: error.message });
    }
  }
  
  return results;
}
```

## Resources

### Tools
- [Rhubarb Lip Sync GitHub](https://github.com/DanielSWolf/rhubarb-lip-sync)
- [Gentle Forced Aligner GitHub](https://github.com/lowerquality/gentle)
- [GentleJS (JavaScript Port)](https://github.com/gmeeker/gentlejs)

### References
- [Preston Blair Phoneme Set](https://en.wikipedia.org/wiki/Preston_Blair)
- [ARPAbet Phoneme Set](http://www.speech.cs.cmu.edu/cgi-bin/cmudict)
- [Viseme Documentation](https://docs.microsoft.com/en-us/azure/cognitive-services/speech-service/how-to-speech-synthesis-viseme)
- [Kaldi Speech Recognition](https://kaldi-asr.org/)

## License

MIT

---

**Project Structure:**
```
speaking-dax/
‚îú‚îÄ‚îÄ README.md                 # This file
‚îú‚îÄ‚îÄ index.html               # Interactive demo
‚îú‚îÄ‚îÄ package.json             # Node dependencies
‚îú‚îÄ‚îÄ utilities/               # Shared utilities
‚îÇ   ‚îî‚îÄ‚îÄ phoneme-to-viseme.js # Phoneme mapping utility
‚îú‚îÄ‚îÄ lip-sync-libraries/      # Lip sync tools
‚îÇ   ‚îú‚îÄ‚îÄ hybrid/             # Hybrid Pipeline (WhisperX ‚Üí Gentle) ‚≠ê
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ hybrid-to-viseme.js
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ convert.sh      # Convenience wrapper script
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ README.md       # Detailed documentation
‚îÇ   ‚îú‚îÄ‚îÄ rhubarb/            # Rhubarb Lip Sync
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ mp3-to-viseme.js
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ convert.sh      # Convenience wrapper script
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Rhubarb-Lip-Sync-1.13.0-macOS/
‚îÇ   ‚îú‚îÄ‚îÄ whisperx/           # WhisperX
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ whisperx-to-viseme.js
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ run_whisperx.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ README.md
‚îÇ   ‚îî‚îÄ‚îÄ gentle/             # Gentle forced aligner
‚îÇ       ‚îú‚îÄ‚îÄ gentle-to-viseme.js
‚îÇ       ‚îî‚îÄ‚îÄ gentle-install/
‚îú‚îÄ‚îÄ audio-samples/           # Sample audio files
‚îî‚îÄ‚îÄ mouth-shapes-png/        # Mouth shape images
```

